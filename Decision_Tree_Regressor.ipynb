{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T08:11:33.277511Z",
     "start_time": "2019-03-17T08:11:33.274205Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Break down of functions\n",
    "\n",
    "* __init__: <br>\n",
    "\n",
    "> - This function initializes a tree with x (features matrix) <br>\n",
    "- y (the label) <br>\n",
    "- min_leaf (minimum number of samples in leaf node) <br>\n",
    "- n (total number of rows) <br>\n",
    "- c (number of columns) <br>\n",
    "- val (mean of label at a node) <br>\n",
    "- score (initialize with inf ) -- stores the variance of resulting split <br>\n",
    "- find_varsplit() -- splits the tree and create left and right subtrees <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T05:19:37.230746Z",
     "start_time": "2019-03-17T05:19:37.225329Z"
    }
   },
   "source": [
    "* __find_better_split__: Find the best split for a single feature column in `self.x` <br>\n",
    "\n",
    "> - set x and y to feature column and label\n",
    " - sort the values of x and corresponding y\n",
    " - `_cnt` stores the number of data points in a node\n",
    " - `_sum` stores sum of labels in a node\n",
    " - `sum2` stores square of sum of labels in a node\n",
    " - `_sum` and `sum2` are used for variance calculation\n",
    ">> - Initially all the elements are assigned to the right node\n",
    "    - Then fill left node with atleast `min_leaf` number of elements and calculate the weighted resulting variance across left and right node for every single element added from right to left node\n",
    "    - Store the split that gives the lowest resulting variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T05:49:22.545607Z",
     "start_time": "2019-03-17T05:49:22.540339Z"
    }
   },
   "source": [
    "* __find_varsplit__: \n",
    "\n",
    ">- Check for the best split and score(variance of left+right) across all columns\n",
    "- Initialize the left and right node with the resulting data of split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T07:47:41.015462Z",
     "start_time": "2019-03-17T07:47:41.006832Z"
    }
   },
   "source": [
    "* __split_col__:\n",
    ">- returns a column that was chosen by find_better_split\n",
    "* __split_name__:\n",
    ">- returns the column name that was that was chosen by find_better_split\n",
    "* __predict__:\n",
    ">- Predict output by passing each data point through predict_row, that returns the mean of observations in the leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T08:05:41.936033Z",
     "start_time": "2019-03-17T08:05:41.924830Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, x, y, idxs = None, min_leaf=2):\n",
    "        if idxs is None: idxs=np.arange(len(y))\n",
    "        self.x,self.y,self.idxs,self.min_leaf = x,y,idxs,min_leaf\n",
    "        self.n,self.c = len(idxs), x.shape[1]\n",
    "        self.val = np.mean(y[idxs])\n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        for i in range(self.c): self.find_better_split(i)\n",
    "        if self.score == float('inf'): return\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x<=self.split)[0]\n",
    "        rhs = np.nonzero(x>self.split)[0]\n",
    "        self.lhs = DecisionTree(self.x, self.y, self.idxs[lhs])\n",
    "        self.rhs = DecisionTree(self.x, self.y, self.idxs[rhs])\n",
    "\n",
    "    def find_better_split(self, var_idx):\n",
    "        x,y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
    "        sort_idx = np.argsort(x)\n",
    "        sort_y,sort_x = y[sort_idx], x[sort_idx]\n",
    "        rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "        lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
    "\n",
    "        for i in range(0,self.n-self.min_leaf-1):\n",
    "            xi,yi = sort_x[i],sort_y[i]\n",
    "            lhs_cnt += 1; rhs_cnt -= 1\n",
    "            lhs_sum += yi; rhs_sum -= yi\n",
    "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "            if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "                continue\n",
    "\n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "            if curr_score<self.score: \n",
    "                self.var_idx,self.score,self.split = var_idx,curr_score,xi\n",
    "\n",
    "    @property\n",
    "    def split_name(self): return self.x.columns[self.var_idx]\n",
    "    \n",
    "    @property\n",
    "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = f'n: {self.n}; val:{self.val}'\n",
    "        if not self.is_leaf:\n",
    "            s += f'; score:{self.score}; split:{self.split}; var:{self.split_name}'\n",
    "        return s\n",
    "\n",
    "    def predict(self, x):\n",
    "        if isinstance(x, pd.DataFrame):\n",
    "            x = x.values\n",
    "        \n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
    "        return t.predict_row(xi)\n",
    "\n",
    "    \n",
    "def std_agg(cnt, s1, s2): \n",
    "    return (s2/cnt) - (s1/cnt)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T08:05:42.761156Z",
     "start_time": "2019-03-17T08:05:42.747309Z"
    }
   },
   "outputs": [],
   "source": [
    "db = load_diabetes()\n",
    "df = pd.DataFrame(data = db.data, columns= db.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T08:05:43.295264Z",
     "start_time": "2019-03-17T08:05:43.156675Z"
    }
   },
   "outputs": [],
   "source": [
    "dt_r = DecisionTree(df,db.target,np.array(range(len(db.target))),min_leaf=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__repr__` returns this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T08:14:01.406198Z",
     "start_time": "2019-03-17T08:14:01.396571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "783.8222473604827"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dt_r.predict(df)\n",
    "np.var(y_pred - db.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T08:13:03.949118Z",
     "start_time": "2019-03-17T08:13:03.938504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=3,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_dt = DecisionTreeRegressor(min_samples_leaf=3)\n",
    "sk_dt.fit(df, db.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T08:13:57.498334Z",
     "start_time": "2019-03-17T08:13:57.491887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "783.8222473604827"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_sk = sk_dt.predict(df)\n",
    "np.var(y_pred_sk - db.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty accurate !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is borrowed from this amazing post https://www.kaggle.com/grroverpr/gradient-boosting-simplified/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
